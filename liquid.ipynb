{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87b0bd2b-8d6d-4217-a1e6-ecc08bb3f157",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "228acdc0-9757-4b27-9e8f-b9ee1b469f1e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "table_size = 12.5\n",
    "\n",
    "number_unique_clients = 15_000 # this is the column to be used as z_order\n",
    "client_ids_to_use = [f'_{i:09}' for i in range(number_unique_clients)]\n",
    "\n",
    "number_cols_in_table = 10\n",
    "\n",
    "SCHEMA_NAME = 'liquid'\n",
    "PREFIX_TRADITIONAL = 'partition_zorder'\n",
    "PREFIX_CLUSTER = 'liquid_cluster'\n",
    "PREFIX_CLUSTER_INVERTED = 'liquid_cluster_inverted'\n",
    "PREFIX_SOURCE = 'source'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "523cb7d5-b6e1-4ec3-af7f-09df01484005",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_dummy_df(\n",
    "    size,\n",
    "    partition_multiplier,\n",
    "    clients_choose,\n",
    "    number_cols\n",
    "):\n",
    "    number_rows = int(size * 1_024 ** 3 / (3 + 10 + 8 * number_cols))\n",
    "    print(f'Number rows to insert: {number_rows:,}')\n",
    "\n",
    "    number_parittions_to_set = max(round(size * partition_multiplier), 1)\n",
    "    print(f'Number parititions: {number_parittions_to_set:,}')\n",
    "\n",
    "    begin_date = datetime(2000, 1, 1)\n",
    "    end_date = begin_date + timedelta(days = number_parittions_to_set)\n",
    "    dates_choose = np.arange(begin_date, end_date, timedelta(days = 1)).astype(datetime)\n",
    "\n",
    "    df = {'col_double_' + str(i): np.random.rand(number_rows) for i in range(1, number_cols + 1)}\n",
    "    df['reference_date'] = np.random.choice(dates_choose, number_rows)\n",
    "    df['client_id'] = np.random.choice(clients_choose, number_rows)\n",
    "\n",
    "    df = pd.DataFrame(df)\n",
    "\n",
    "    df['reference_date'] = df['reference_date'].dt.date\n",
    "\n",
    "    df = spark.createDataFrame(df)\n",
    "\n",
    "    return df\n",
    "\n",
    "def describe_table(\n",
    "    table_name,\n",
    "):\n",
    "    table_details = spark.sql(f'DESCRIBE DETAIL {SCHEMA_NAME}.{table_name}').collect()\n",
    "\n",
    "    # print(f'Size: {table_details[0][\"sizeInBytes\"] / (1_024 ** 3):,.0f} GB')\n",
    "    print(f'{table_details[0][\"numFiles\"]:,} files')\n",
    "    print(f'Partition cols: {table_details[0][\"partitionColumns\"]}')\n",
    "    print(f'Clustering cols: {table_details[0][\"clusteringColumns\"]}')\n",
    "\n",
    "def optimize_table(table_name, with_zorder):\n",
    "    additional = 'ZORDER BY (client_id)' if with_zorder else ''\n",
    "    print(additional)\n",
    "    spark.sql(f'OPTIMIZE {SCHEMA_NAME}.{table_name} {additional}')\n",
    "\n",
    "def create_table(\n",
    "    prefix,\n",
    "    suffix,\n",
    "    size,\n",
    "    number_cols,\n",
    "):\n",
    "    if prefix == PREFIX_TRADITIONAL:\n",
    "        table_name = prefix + '_' + str(size).replace('.', '_') + 'GB_' + suffix\n",
    "        partition_strategy = 'PARTITIONED BY (reference_date)'\n",
    "    elif prefix == PREFIX_CLUSTER:\n",
    "        table_name = prefix + '_' + str(size).replace('.', '_') + 'GB_' + suffix\n",
    "        partition_strategy = 'CLUSTER BY (reference_date, client_id)'\n",
    "    elif prefix == PREFIX_CLUSTER_INVERTED:\n",
    "        table_name = prefix + '_' + str(size).replace('.', '_') + 'GB_' + suffix\n",
    "        partition_strategy = 'CLUSTER BY (client_id, reference_date)'\n",
    "    elif prefix == PREFIX_SOURCE:\n",
    "        table_name = prefix + '_' + str(size).replace('.', '_') + 'GB_' + suffix\n",
    "        partition_strategy = ''\n",
    "    else:\n",
    "        print(f'wrong prefix: {prefix}')\n",
    "        return\n",
    "\n",
    "    cols = [\n",
    "        'col_double_' + str(i) + ' DOUBLE'\n",
    "        for i in range(1, number_cols + 1)\n",
    "    ]\n",
    "    cols = ', '.join(cols)\n",
    "\n",
    "    spark.sql(\n",
    "        f'''\n",
    "\n",
    "        CREATE OR REPLACE TABLE {SCHEMA_NAME}.{table_name}\n",
    "        (\n",
    "            reference_date DATE,\n",
    "            client_id STRING,\n",
    "            {cols}\n",
    "        )\n",
    "        USING delta\n",
    "        {partition_strategy}\n",
    "\n",
    "        '''\n",
    "    )\n",
    "\n",
    "    with_zorder = prefix == PREFIX_TRADITIONAL\n",
    "    optimize_table(table_name, with_zorder)\n",
    "\n",
    "    return table_name\n",
    "\n",
    "def ingest_values(\n",
    "    table_name,\n",
    "    prefix,\n",
    "):\n",
    "    \n",
    "    start = time.time()\n",
    "    (\n",
    "        spark.table(SCHEMA_NAME + '.' + table_name.replace(prefix, PREFIX_SOURCE))\n",
    "        .write\n",
    "        .mode('append')\n",
    "        .saveAsTable(SCHEMA_NAME + '.' + table_name)\n",
    "    )\n",
    "    end = time.time()\n",
    "    total_time_seconds = end - start\n",
    "\n",
    "    return total_time_seconds\n",
    "\n",
    "def create_source(\n",
    "    suffix,\n",
    "    table_size,\n",
    "    number_cols_in_table,\n",
    "    client_ids_to_use,\n",
    "    partition_multiplier,\n",
    "):\n",
    "    source_table = create_table(PREFIX_SOURCE, suffix, table_size, number_cols_in_table)\n",
    "\n",
    "    df = create_dummy_df(table_size, partition_multiplier, client_ids_to_use, number_cols_in_table)\n",
    "\n",
    "    (\n",
    "        df\n",
    "        .write\n",
    "        .mode('append')\n",
    "        .saveAsTable(SCHEMA_NAME + '.' + source_table)\n",
    "    )\n",
    "\n",
    "    df = []\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0e8d92d-eb75-4a8b-bff5-98ee4df0d96d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "create_source('ingest', table_size, number_cols_in_table, client_ids_to_use, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f6d3fa3-b15f-4bba-b66f-a167921980f9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ingestion_results = {}\n",
    "\n",
    "for table_suffix in [PREFIX_TRADITIONAL, PREFIX_CLUSTER, PREFIX_CLUSTER_INVERTED]:\n",
    "\n",
    "    created_table = create_table(table_suffix, 'ingest', table_size, number_cols_in_table)\n",
    "\n",
    "    describe_table(created_table)\n",
    "\n",
    "    total_time = ingest_values(created_table, table_suffix)\n",
    "\n",
    "    ingestion_results[created_table] = total_time\n",
    "\n",
    "    print(table_suffix, table_size, f'{total_time:,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f064a72d-debc-42a8-bd79-90a1fb17210d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ingestion_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f3df2ce-ca10-42e9-babd-e27f718cb158",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "client_to_search = client_ids_to_use[np.random.randint(0, len(client_ids_to_use) - 1)]\n",
    "client_to_search = \"'\" + client_to_search + \"'\"\n",
    "\n",
    "available_dates = (\n",
    "    spark\n",
    "    .table(SCHEMA_NAME + '.' + list(ingestion_results.keys())[0])\n",
    "    .select('reference_date')\n",
    "    .distinct()\n",
    "    .collect()\n",
    ")\n",
    "available_dates = [dt[0] for dt in available_dates]\n",
    "\n",
    "if len(available_dates) > 1:\n",
    "    date_to_search = available_dates[np.random.randint(0, len(available_dates) - 1)]\n",
    "else:\n",
    "    date_to_search = available_dates[0]\n",
    "date_to_search = \"'\" + f'{date_to_search:%Y-%m-%d}' + \"'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99b1f70b-29a7-4ae7-be8c-d8486dc8231a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for table_name in ingestion_results:\n",
    "    with_zorder = table_name.startswith(PREFIX_TRADITIONAL)\n",
    "    optimize_table(table_name, with_zorder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3146770e-54d3-4a17-863b-7da42e6a14b4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "read_zorder_col_results = {}\n",
    "read_partition_col_results = {}\n",
    "\n",
    "for table_name in ingestion_results:\n",
    "\n",
    "    start = time.time()\n",
    "    res = (\n",
    "        spark\n",
    "        .sql(\n",
    "            f'''\n",
    "\n",
    "            SELECT\n",
    "                *\n",
    "            FROM\n",
    "                {SCHEMA_NAME}.{table_name}\n",
    "            WHERE\n",
    "                client_id == {client_to_search}\n",
    "            LIMIT 1000\n",
    "            '''\n",
    "        )\n",
    "        .collect()\n",
    "    )\n",
    "    end = time.time()\n",
    "    total_time_seconds = end - start\n",
    "\n",
    "    read_zorder_col_results[table_name] = total_time_seconds\n",
    "\n",
    "    start = time.time()\n",
    "    res = (\n",
    "        spark\n",
    "        .sql(\n",
    "            f'''\n",
    "\n",
    "            SELECT\n",
    "                *\n",
    "            FROM\n",
    "                {SCHEMA_NAME}.{table_name}\n",
    "            WHERE\n",
    "                reference_date == {date_to_search}\n",
    "            LIMIT 1000\n",
    "            '''\n",
    "        )\n",
    "        .collect()\n",
    "    )\n",
    "    end = time.time()\n",
    "    total_time_seconds = end - start\n",
    "\n",
    "    read_partition_col_results[table_name] = total_time_seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9aceb60b-6ae4-4c0e-b7da-38a6c67d05fa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "read_partition_col_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9c1d107-4c34-4299-955e-af1e27301880",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "read_zorder_col_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca4afd89-91ed-4471-a00f-34da6d39857b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "create_source('mult_part', table_size, number_cols_in_table, client_ids_to_use, 2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "734f98c0-7d4d-4d09-86bf-77d217052a06",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ingestion_part_results = {}\n",
    "\n",
    "for table_suffix in [PREFIX_TRADITIONAL, PREFIX_CLUSTER, PREFIX_CLUSTER_INVERTED]:\n",
    "\n",
    "    created_table = create_table(table_suffix, 'mult_part', table_size, number_cols_in_table)\n",
    "\n",
    "    describe_table(created_table)\n",
    "\n",
    "    total_time = ingest_values(created_table, table_suffix)\n",
    "\n",
    "    ingestion_part_results[created_table] = total_time\n",
    "\n",
    "    print(table_suffix, table_size, f'{total_time:,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17ba1bef-823e-4d94-bca1-c818b17e7e58",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ingestion_part_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cda07d9b-e8ad-4a32-9a7a-b69333c20f66",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "number_unique_clients = 75_000 # this is the column to be used as z_order\n",
    "client_ids_to_use = [f'_{i:09}' for i in range(number_unique_clients)]\n",
    "\n",
    "create_source('mult_zorder', table_size, number_cols_in_table, client_ids_to_use, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "180dedc8-abd1-4de1-a40f-d0d8358ca97f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ingestion_zorder_results = {}\n",
    "\n",
    "for table_suffix in [PREFIX_TRADITIONAL, PREFIX_CLUSTER, PREFIX_CLUSTER_INVERTED]:\n",
    "\n",
    "    created_table = create_table(table_suffix, 'mult_zorder', table_size, number_cols_in_table)\n",
    "\n",
    "    describe_table(created_table)\n",
    "\n",
    "    total_time = ingest_values(created_table, table_suffix)\n",
    "\n",
    "    ingestion_zorder_results[created_table] = total_time\n",
    "\n",
    "    print(table_suffix, table_size, f'{total_time:,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "846c91be-e683-439d-b03e-9cad639cb3db",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ingestion_zorder_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "336474f0-83c7-4df6-a807-7734ff2f5346",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3bfff5b-b81e-4fd7-aed6-28d94c358517",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 267931748798075,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "liquid",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
